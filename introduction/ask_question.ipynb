{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first tutorial on LlamaIndex to aks questions from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1\" 200 6800\n",
      "https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1\" 200 6800\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6800\n",
      "https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6800\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "DEBUG:llama_index.core.readers.file.base:> [SimpleDirectoryReader] Total files added: 3\n",
      "> [SimpleDirectoryReader] Total files added: 3\n",
      "DEBUG:fsspec.local:open file: /Users/koyiljonvaliev/Desktop/LlamaIndex/LlamaIndex_Tutorial/introduction/data/Koyiljon_CV.pdf\n",
      "open file: /Users/koyiljonvaliev/Desktop/LlamaIndex/LlamaIndex_Tutorial/introduction/data/Koyiljon_CV.pdf\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "DEBUG:fsspec.local:open file: /Users/koyiljonvaliev/Desktop/LlamaIndex/LlamaIndex_Tutorial/introduction/data/Koyiljon_Portfolio.pdf\n",
      "open file: /Users/koyiljonvaliev/Desktop/LlamaIndex/LlamaIndex_Tutorial/introduction/data/Koyiljon_Portfolio.pdf\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "DEBUG:fsspec.local:open file: /Users/koyiljonvaliev/Desktop/LlamaIndex/LlamaIndex_Tutorial/introduction/data/a.txt\n",
      "open file: /Users/koyiljonvaliev/Desktop/LlamaIndex/LlamaIndex_Tutorial/introduction/data/a.txt\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Summary \n",
      "Koyilbek Valiev  \n",
      "E-mail: valievkoyilj...\n",
      "> Adding chunk: Summary \n",
      "Koyilbek Valiev  \n",
      "E-mail: valievkoyilj...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: • Managed comprehensive technical documentaSon ...\n",
      "> Adding chunk: • Managed comprehensive technical documentaSon ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: enabling video-based querying via both text-bas...\n",
      "> Adding chunk: enabling video-based querying via both text-bas...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: PORTFOLIO\n",
      "1. Multimodal Effective Video Search ...\n",
      "> Adding chunk: PORTFOLIO\n",
      "1. Multimodal Effective Video Search ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 2. LLaMA Customer Support Assistant: Fine-tuned...\n",
      "> Adding chunk: 2. LLaMA Customer Support Assistant: Fine-tuned...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Technical Strategy:\n",
      "• Advanced Architecture: Bu...\n",
      "> Adding chunk: Technical Strategy:\n",
      "• Advanced Architecture: Bu...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Results:\n",
      "● Efﬁcient schedule generation with en...\n",
      "> Adding chunk: Results:\n",
      "● Efﬁcient schedule generation with en...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Project Link\n",
      "7. Stable Diffusion Image Generati...\n",
      "> Adding chunk: Project Link\n",
      "7. Stable Diffusion Image Generati...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 1. F1: Discharge Time (s)\n",
      "2. F4: Decrement 3.6-...\n",
      "> Adding chunk: 1. F1: Discharge Time (s)\n",
      "2. F4: Decrement 3.6-...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Size Estimation:  Providing estimations of tras...\n",
      "> Adding chunk: Size Estimation:  Providing estimations of tras...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: This project is dedicated to an innovative IoT-...\n",
      "> Adding chunk: This project is dedicated to an innovative IoT-...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Utilising PyTorch and machine learning, this pr...\n",
      "> Adding chunk: Utilising PyTorch and machine learning, this pr...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college...\n",
      "> Adding chunk: What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: All that seemed left for philosophy were edge c...\n",
      "> Adding chunk: All that seemed left for philosophy were edge c...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Its brokenness did, as so often happens, genera...\n",
      "> Adding chunk: Its brokenness did, as so often happens, genera...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: If he even knew about the strange classes I was...\n",
      "> Adding chunk: If he even knew about the strange classes I was...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The students and faculty in the painting depart...\n",
      "> Adding chunk: The students and faculty in the painting depart...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I wanted to go back to RISD, but I was now brok...\n",
      "> Adding chunk: I wanted to go back to RISD, but I was now brok...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: But alas it was more like the Accademia than no...\n",
      "> Adding chunk: But alas it was more like the Accademia than no...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: After I moved to New York I became her de facto...\n",
      "> Adding chunk: After I moved to New York I became her de facto...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Now we felt like we were really onto something....\n",
      "> Adding chunk: Now we felt like we were really onto something....\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I kept the code tight and didn't have to integr...\n",
      "> Adding chunk: I kept the code tight and didn't have to integr...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: A company with just a handful of employees woul...\n",
      "> Adding chunk: A company with just a handful of employees woul...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I stuck it out for a few more months, then in d...\n",
      "> Adding chunk: I stuck it out for a few more months, then in d...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: But about halfway through the summer I realized...\n",
      "> Adding chunk: But about halfway through the summer I realized...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Still life has always been the least prestigiou...\n",
      "> Adding chunk: Still life has always been the least prestigiou...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: and went on with the talk. But afterward it occ...\n",
      "> Adding chunk: and went on with the talk. But afterward it occ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: We knew undergrads were deciding then about sum...\n",
      "> Adding chunk: We knew undergrads were deciding then about sum...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: HN was no doubt good for YC, but it was also by...\n",
      "> Adding chunk: HN was no doubt good for YC, but it was also by...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The stroke destroyed her balance, and she was p...\n",
      "> Adding chunk: The stroke destroyed her balance, and she was p...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The Lisp that John McCarthy invented, or more a...\n",
      "> Adding chunk: The Lisp that John McCarthy invented, or more a...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: In the summer of 2016 we moved to England. We w...\n",
      "> Adding chunk: In the summer of 2016 we moved to England. We w...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: [9] We'd had a code editor in Viaweb for users ...\n",
      "> Adding chunk: [9] We'd had a code editor in Viaweb for users ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: When you run a forum, you're assumed to see if ...\n",
      "> Adding chunk: When you run a forum, you're assumed to see if ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8af4671bc4473eb66a8ffcc156f462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ef0feab2964938b062d777a5c8d8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd6f5ab35a843dfa75b2e6cc2495ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8951f690e9242eba17589d02f4aa677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Disable LLM \n",
    "Settings.llm = None\n",
    "\n",
    "# embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Set as default embedding model\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# index with local embeddings\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422eb3399c5a42149e36ed1622e98a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.indices.utils:> Top 2 nodes:\n",
      "> [Node a45015c2-3fb5-4f58-9de3-92afb55cf13d] [Similarity score:             0.358988] Summary \n",
      "Koyilbek Valiev  \n",
      "E-mail: valievkoyiljon112@gmail.com       Phone:   +(82)-010-2253-3010...\n",
      "> [Node 8d37d1fa-b347-4871-ad10-90726b2a36fc] [Similarity score:             0.300511] PORTFOLIO\n",
      "1. Multimodal Effective Video Search and Interaction with RAG\n",
      "As part of the Multimodal...\n",
      "> Top 2 nodes:\n",
      "> [Node a45015c2-3fb5-4f58-9de3-92afb55cf13d] [Similarity score:             0.358988] Summary \n",
      "Koyilbek Valiev  \n",
      "E-mail: valievkoyiljon112@gmail.com       Phone:   +(82)-010-2253-3010...\n",
      "> [Node 8d37d1fa-b347-4871-ad10-90726b2a36fc] [Similarity score:             0.300511] PORTFOLIO\n",
      "1. Multimodal Effective Video Search and Interaction with RAG\n",
      "As part of the Multimodal...\n",
      "Context information is below.\n",
      "---------------------\n",
      "page_label: 1\n",
      "file_path: /Users/koyiljonvaliev/Desktop/LlamaIndex/LlamaIndex_Tutorial/introduction/data/Koyiljon_CV.pdf\n",
      "\n",
      "Summary \n",
      "Koyilbek Valiev  \n",
      "E-mail: valievkoyiljon112@gmail.com       Phone:   +(82)-010-2253-3010 \n",
      "Github: github.com/koyilbek     Website: koyiljon.github.io \n",
      "Hello! I’m Koyilbek from Uzbekistan, a passionate AI engineer dedicated to building intelligent systems and \n",
      "leveraging cuNng-edge technologies to solve real-world problems. With over three years of experience in diverse, \n",
      "hands-on projects, I bring a proven ability to tackle challenges and drive impacRul soluSons. I am deeply commiTed \n",
      "to conSnuous learning and growth, currently expanding my exper Sse in mul Smodal AI, including Large Language \n",
      "Models (LLMs) and Large Vision Models (LVMs). I thrive on crea Sng innovaSve soluSons that contribute to societal \n",
      "progress.   \n",
      "Experiences \n",
      "AI Research Intern                                                                                                       Recs Innova+on Ltd, Naju, South Korea                      \n",
      "July 2024 – Present \n",
      "Op;mal Bidding Predic;on: \n",
      "• Engineered an ML-based bidding opSmizaSon system for energy projects integraSng supervised learning and \n",
      "Sme series analysis: \n",
      " - Built comprehensive data pipeline with automated cleaning, anomaly detecSon, and feature engineering. \n",
      " - Achieved 0.0017 % predicSon error rate through Hybrid ML algorithms based on data distribuSon. \n",
      "• Increased bid winning probability by 15x . \n",
      "Solar Power Genera;on Predic;on Project:\n",
      "• Developed an end-to-end solar power forecasSng system achieving 6-8% forecast error rate through hyper \n",
      "parameter tuning and weather forecast data integraSon. \n",
      "• Designed and implemented a comprehensive predicSon framework uSlizing: \n",
      "- Advanced neural networks: Custom LSTM-CNN hybrid architecture, GRU with aTenSon mechanisms, and \n",
      "transformer-based models. \n",
      "- Classical ML models: OpSmized implementaSons of Linear Regression, XGBoost, and Random Forest \n",
      "algorithms. \n",
      "- Reinforcement learning: Developed custom OpenAI Gym environments to implement reinforcement \n",
      "learning algorithms (PPO, A2C, DDPG) with PyTorch and Stable-Baselines3 for power predicSon.  \n",
      "• Successfully integrated the opSmized predicSon model into Sun-Q Energy Management System (EMS), enabling \n",
      "real-Sme power generaSon forecasSng capabiliSes. \n",
      "AI  Lead  Intern                                                                                                              Recs Innova+on Ltd, Naju, South Korea  \n",
      "March 2024 – July 2024 (5 months)\n",
      "• Led development of an advanced unsupervised anomaly detecSon system for photovoltaic sensors, \n",
      "successfully integrated into Sun-Q Energy Management System (EMS). \n",
      "• Implemented state-of-the-art deep learning architectures (LSTM Autoencoder, LSTM-VAE, TranAD) while \n",
      "overseeing team development of GNN and USAD models. \n",
      "• Built automated data pipeline processing 1.3M+ daily sensor readings, reducing preprocessing Sme from 3 \n",
      "hours to 15 minutes. \n",
      "• Established MLﬂow experimentaSon framework for systemaSc model evaluaSon and opSmizaSon, tracking \n",
      "key performance metrics (recall, precision, F1-score)  across mulSple architectures.\n",
      "\n",
      "page_label: 1\n",
      "file_path: /Users/koyiljonvaliev/Desktop/LlamaIndex/LlamaIndex_Tutorial/introduction/data/Koyiljon_Portfolio.pdf\n",
      "\n",
      "PORTFOLIO\n",
      "1. Multimodal Effective Video Search and Interaction with RAG\n",
      "As part of the Multimodal RAG: Chat with Videos course by DeepLearning.AI in collaboration with \n",
      "Intel, I developed a Multimodal Retrieval-Augmented Generation (RAG) system designed to enable \n",
      "intelligent querying and interaction with video content. This system leverages multimodal AI \n",
      "techniques including video frame extraction, transcription, multimodal embedding, and retrieval.  \n",
      "Technical Strategy:\n",
      "• Video Preprocessing: Developed video preprocessing scripts to handle three different video \n",
      "input types (video with transcript, video with audio but no transcript, and video with no audio \n",
      "or transcript). This included extracting video frames and generating transcriptions using \n",
      "OpenAI's Whisper and LLaV A models.\n",
      "• Vector Store Ingestion: Implemented ingestion into LanceDB vector database using \n",
      "BridgeTower embeddings. Experimented with augmenting fragmented transcripts to improve \n",
      "the quality of the data used for retrieval.\n",
      "• RAG System Implementation: Used LangChain to create a multimodal retrieval-augmented \n",
      "generation (RAG) system, enabling video-based querying and interaction. Implemented both \n",
      "text-based and web interface querying systems using Python and Gradio.\n",
      "Project Deliverables:\n",
      "• Video Frame Extraction: Developed a system that extracts frames from videos and associates \n",
      "them with metadata such as transcript and video segment information.\n",
      "• Multimodal Data Ingestion: Processed and ingested video and transcript data into a vector \n",
      "store to make it searchable.\n",
      "• RAG System: Enabled users to ask multimodal queries about video content using a \n",
      "combination of video frames, transcripts, and embeddings.\n",
      "Tools Used:\n",
      "• LangChain: To run multimodal RAG system\n",
      "• OpenAI Whisper: For transcription\n",
      "• MoviePy: For audio extraction from videos\n",
      "• LLaV A Model: For generating captions from video frames\n",
      "• LanceDB: For vector storage\n",
      "API Keys Used:\n",
      "• OpenAI API: For natural language processing\n",
      "• Prediction Guard API: For model predictions\n",
      "ML Algorithms:\n",
      "• BridgeTower Embeddings: For vectorizing transcripts\n",
      "• RAG System: For querying video data\n",
      "View Certiﬁcate of Completion  |  Project Link\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query:  Please tell me about projects of Koyilbek related to LLM and RAG applications. Please give information about Yolo8 related projects on portfolio\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "#Query\n",
    "response = query_engine.query(\" Please tell me about projects of Koyilbek related to LLM and RAG applications. Please give information about Yolo8 related projects on portfolio\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaIndex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
