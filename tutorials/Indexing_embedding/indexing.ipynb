{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]\n",
      "Parsing nodes: 100%|██████████| 40/40 [00:00<00:00, 501.74it/s]\n",
      "Generating embeddings: 100%|██████████| 83/83 [00:03<00:00, 25.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex , SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# local embedding\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# local LLM\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/phi-2\",  # This is a smaller model that works well for most tasks\n",
    "    tokenizer_name=\"microsoft/phi-2\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../../data\").load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose to build an index over a list of Node objects directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.ingestion import  IngestionPipeline\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "\n",
    "# loading documents\n",
    "documents = SimpleDirectoryReader(\"../../data\").load_data()\n",
    "\n",
    "# pipeline with text splitter \n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[TokenTextSplitter(), ])\n",
    "\n",
    "# processing documents into nodes\n",
    "nodes =  pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 80/80 [00:03<00:00, 25.11it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(nodes, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "em = index.as_query_engine()\n",
    "response = em.query(\"what is about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n------------\\nGPT models are being used to revolutionize the entertainment industry by providing endless entertainment. Since its evolution, GPT models have been adopted as an entertainer crosschecking their ability to produce content on funny and illogical questions. GPTs entertain people in many ways, and of course, using GPT itself as an entertainer, reducing the burden of overthinking by providing immediate feedback to queries in seconds. The results are amazing, and they have been utilized for many purposes today. Some of the impacts of GPTs on Entertainment applications are given below:\\n• Solitude with GPT: As the GPT itself is an entertainer,\\none can feel better alone with the GPT, which helps to\\ncome out of loneliness by exploring its savors [147]. GPTs\\nassist in providing soothing poems, mental healing\\nquotes, and funny riddles. People with loneliness may feel\\nanxiety, especially with older ones at home. In this case,\\nGPT-4 helps people with its V oice Technology feature,\\nenabling users to input their audio [147]. In turn, the\\nGPT model responds to user-speciﬁc speech output using\\nNLP algorithms embedded with it'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
