{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've loaded your data, built an index, and stored that index for later, you're ready to get to the most significant part of an LLM application: querying.\n",
    "\n",
    "At its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction.\n",
    "\n",
    "More complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "Parsing nodes: 100%|██████████| 40/40 [00:00<00:00, 506.33it/s]\n",
      "Generating embeddings: 100%|██████████| 83/83 [00:03<00:00, 25.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex , SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# local embedding\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# First create the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    padding_side=\"right\",\n",
    "    pad_token=\"<|endoftext|>\"  # Define custom pad token\n",
    ")\n",
    "# Add pad token to tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "\n",
    "# local LLM\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/phi-2\",  # This is a smaller model that works well for most tasks\n",
    "    tokenizer_name=\"microsoft/phi-2\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=1024,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True, \"pad_token_id\": tokenizer.pad_token_id},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "documents = SimpleDirectoryReader(\"../../data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine( \n",
    "                                    # similarity_top_k=1,\n",
    "                                    response_mode=\"tree_summarize\",)\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"write me short essay based on the given document about GPT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT (Generative Pre-trained Transformer) is a language model that has been developed by OpenAI. It is an artificial intelligence system that can generate human-like text based on the input it receives. GPT has been used in various applications, including chatbots, content creation, and research writing.\n",
      "\n",
      "GPT has several advantages in the healthcare sector. It can be used to generate personalized medical reports, which can help healthcare professionals make more informed decisions about patient care. GPT can also be used to generate medical research papers, which can help researchers share their findings more quickly and efficiently. Additionally, GPT can be used to generate patient education materials, which can help patients better understand their medical conditions and treatment options.\n",
      "\n",
      "However, there are also challenges to consider when using GPT in healthcare. One challenge is ensuring the accuracy and reliability of the information generated by GPT. Another challenge is ensuring the privacy and security of patient data. Finally, there is a need for human oversight to ensure that the information generated by GPT is appropriate and ethical.\n",
      "\n",
      "In conclusion, GPT has the potential to revolutionize the healthcare sector by improving the efﬁciency and quality of healthcare services. However, there are also challenges to consider when using GPT in healthcare, including ensuring the accuracy and reliability of the information generated by GPT and ensuring the privacy and security of patient data.\n",
      "\n",
      "Follow-up exercises:\n",
      "1) How can GPT be used to generate personalized medical reports?\n",
      "2) What are some potential ethical concerns associated with using GPT in healthcare?\n",
      "3) How can GPT be used to generate patient education materials?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stages of querying\n",
    "However, there is more to querying than initially meets the eye. Querying consists of three distinct stages:\n",
    "\n",
    "- <b>Retrieval</b> is when you find and return the most relevant documents for your query from your Index. As previously discussed in indexing, the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies.\n",
    "- <b>Postprocessing</b> is when the Nodes retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached.\n",
    "- <b>Response synthesis</b> is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the stages of querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex features a low-level composition API that gives you granular control over your querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we customize our retriever to use a different number for top_k and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex , SimpleDirectoryReader, Settings, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# local embedding\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# First create the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    padding_side=\"right\",\n",
    "    pad_token=\"<|endoftext|>\"  # Define custom pad token\n",
    ")\n",
    "# Add pad token to tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "\n",
    "# local LLM\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/phi-2\",  # This is a smaller model that works well for most tasks\n",
    "    tokenizer_name=\"microsoft/phi-2\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=1024,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True, \"pad_token_id\": tokenizer.pad_token_id},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "documents = SimpleDirectoryReader(\"../../data\").load_data()\n",
    "\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What does document about \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Phi3.5 instruction model instead of Phi2 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No sentence-transformers model found with name microsoft/Phi-3.5-mini-instruct. Creating a new one with mean pooling.\n",
      "c:\\Users\\User\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\AppData\\Local\\llama_index\\models--microsoft--Phi-3.5-mini-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [03:01<00:00, 90.98s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "c:\\Users\\User\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--microsoft--Phi-3.5-mini-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [03:02<00:00, 91.38s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Advantages and Disadvantages of GPT\n",
      "\n",
      "Generative Pre-trained Transformers (GPT) have revolutionized the field of natural language processing (NLP) and have become a cornerstone in the development of AI-driven applications. These models offer a plethora of advantages, but they also come with certain disadvantages that need to be considered.\n",
      "\n",
      "**Advantages of GPT:**\n",
      "\n",
      "1. **Natural Language Understanding and Generation:**\n",
      "   GPT models excel at understanding and generating human-like text, making them highly effective for a wide range of applications, including chatbots, content creation, and language translation.\n",
      "\n",
      "2. **Data Efficiency:**\n",
      "   GPT models can be fine-tuned with relatively small datasets, which is beneficial for organizations with limited data resources.\n",
      "\n",
      "3. **Multimodal Capabilities:**\n",
      "   Some GPT models, like GPT-3, have multimodal capabilities, allowing them to understand and generate text based on visual inputs, which is a significant step towards more comprehensive AI systems.\n",
      "\n",
      "4. **Scalability:**\n",
      "   GPT models are highly scalable, meaning they can be trained on larger datasets to improve their performance and accuracy.\n",
      "\n",
      "5. **Versatility:**\n",
      "   GPT models can be adapted for various tasks, from simple text generation to complex problem-solving, making them versatile tools for developers.\n",
      "\n",
      "6. **Cost-Effectiveness:**\n",
      "   Once trained, GPT models can be deployed at scale with relatively low incremental costs, making them cost-effective for businesses.\n",
      "\n",
      "**Disadvantages of GPT:**\n",
      "\n",
      "1. **Computational Resources:**\n",
      "   Training and running GPT models require significant computational resources, including powerful GPUs and substantial memory, which can be expensive.\n",
      "\n",
      "2. **Ethical Concerns:**\n",
      "   GPT models can inadvertently generate biased or inappropriate content if trained on biased datasets. There are also concerns about the ethical implications of AI-generated content.\n",
      "\n",
      "3. **Lack of Interpretability:**\n",
      "   The \"black box\" nature of deep learning models like GPT makes it difficult to understand how they arrive at certain outputs, which raises concerns about trustworthiness and accountability.\n",
      "\n",
      "4. **Data Privacy:**\n",
      "   GPT models require large amounts of data, which can raise privacy concerns. The data used for training these models may contain sensitive information.\n",
      "\n",
      "5. **Dependency on Quality Data:**\n",
      "   The performance of GPT models is heavily dependent on the quality of the training data. Poor quality data can lead to inaccurate or nonsensical outputs.\n",
      "\n",
      "6. **Potential for Misuse:**\n",
      "   The ability of GPT models to generate convincing text can be exploited for creating fake news, phishing attacks, or other malicious purposes.\n",
      "\n",
      "7. **Environmental Impact:**\n",
      "   The energy consumption required to train and run GPT models contributes to the carbon footprint, raising environmental concerns.\n",
      "\n",
      "8. **Job Displacement:**\n",
      "   The automation capabilities of GPT models may lead to job displacement in certain sectors, particularly those involving content creation and customer service.\n",
      "\n",
      "9. **Limited Contextual Understanding:**\n",
      "   While GPT models are good at generating text based on the input they receive, they may struggle with maintaining context over longer conversations or complex topics.\n",
      "\n",
      "10. **Security Risks:**\n",
      "    GPT models can be vulnerable to adversarial attacks, which could be exploited to manipulate the model or generate harmful content.\n",
      "\n",
      "In conclusion, while GPT models offer substantial benefits in terms of language processing capabilities and versatility, they also present several challenges that need to be addressed. The ethical, privacy, and security implications, along with the environmental impact, must be carefully managed to ensure that the advantages of GPT are realized responsibly.\n",
      "\n",
      "\n",
      "**Note:** The provided context does not contain a direct essay on the advantages and disadvantages of GPT. The above answer is a synthesized response based on general knowledge about GPT models. For a detailed essay, one would need to conduct further research or refer to specific studies and analyses on the topic.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex , SimpleDirectoryReader, Settings, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Use a more advanced embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "# Create the tokenizer for Phi-3.5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    padding_side=\"right\",\n",
    "    pad_token=\"<|endoftext|>\"\n",
    ")\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "\n",
    "# Set up the Phi-3.5 model\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    tokenizer_name=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    context_window=128000,  # Phi-3.5 supports 128K context length\n",
    "    max_new_tokens=1024,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True, \"pad_token_id\": tokenizer.pad_token_id},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"../../data\").load_data()\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"Please write me essay about advantages and disadvantages of GPT? \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a huge variety of retrievers that you can learn about in our module guide on retrievers: https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
