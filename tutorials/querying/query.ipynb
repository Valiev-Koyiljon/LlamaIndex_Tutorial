{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've loaded your data, built an index, and stored that index for later, you're ready to get to the most significant part of an LLM application: querying.\n",
    "\n",
    "At its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction.\n",
    "\n",
    "More complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "Parsing nodes: 100%|██████████| 40/40 [00:00<00:00, 506.33it/s]\n",
      "Generating embeddings: 100%|██████████| 83/83 [00:03<00:00, 25.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex , SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# local embedding\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# First create the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    padding_side=\"right\",\n",
    "    pad_token=\"<|endoftext|>\"  # Define custom pad token\n",
    ")\n",
    "# Add pad token to tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "\n",
    "# local LLM\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/phi-2\",  # This is a smaller model that works well for most tasks\n",
    "    tokenizer_name=\"microsoft/phi-2\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=1024,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True, \"pad_token_id\": tokenizer.pad_token_id},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "documents = SimpleDirectoryReader(\"../../data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine( \n",
    "                                    # similarity_top_k=1,\n",
    "                                    response_mode=\"tree_summarize\",)\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"write me short essay based on the given document about GPT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT (Generative Pre-trained Transformer) is a language model that has been developed by OpenAI. It is an artificial intelligence system that can generate human-like text based on the input it receives. GPT has been used in various applications, including chatbots, content creation, and research writing.\n",
      "\n",
      "GPT has several advantages in the healthcare sector. It can be used to generate personalized medical reports, which can help healthcare professionals make more informed decisions about patient care. GPT can also be used to generate medical research papers, which can help researchers share their findings more quickly and efficiently. Additionally, GPT can be used to generate patient education materials, which can help patients better understand their medical conditions and treatment options.\n",
      "\n",
      "However, there are also challenges to consider when using GPT in healthcare. One challenge is ensuring the accuracy and reliability of the information generated by GPT. Another challenge is ensuring the privacy and security of patient data. Finally, there is a need for human oversight to ensure that the information generated by GPT is appropriate and ethical.\n",
      "\n",
      "In conclusion, GPT has the potential to revolutionize the healthcare sector by improving the efﬁciency and quality of healthcare services. However, there are also challenges to consider when using GPT in healthcare, including ensuring the accuracy and reliability of the information generated by GPT and ensuring the privacy and security of patient data.\n",
      "\n",
      "Follow-up exercises:\n",
      "1) How can GPT be used to generate personalized medical reports?\n",
      "2) What are some potential ethical concerns associated with using GPT in healthcare?\n",
      "3) How can GPT be used to generate patient education materials?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stages of querying\n",
    "However, there is more to querying than initially meets the eye. Querying consists of three distinct stages:\n",
    "\n",
    "- <b>Retrieval</b> is when you find and return the most relevant documents for your query from your Index. As previously discussed in indexing, the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies.\n",
    "- <b>Postprocessing</b> is when the Nodes retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached.\n",
    "- <b>Response synthesis</b> is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the stages of querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex features a low-level composition API that gives you granular control over your querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we customize our retriever to use a different number for top_k and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex , SimpleDirectoryReader, Settings, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# local embedding\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# First create the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    padding_side=\"right\",\n",
    "    pad_token=\"<|endoftext|>\"  # Define custom pad token\n",
    ")\n",
    "# Add pad token to tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "\n",
    "# local LLM\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/phi-2\",  # This is a smaller model that works well for most tasks\n",
    "    tokenizer_name=\"microsoft/phi-2\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=1024,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True, \"pad_token_id\": tokenizer.pad_token_id},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "documents = SimpleDirectoryReader(\"../../data\").load_data()\n",
    "\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What does document about \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Phi3.5 instruction model instead of Phi2 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No sentence-transformers model found with name microsoft/Phi-3.5-mini-instruct. Creating a new one with mean pooling.\n",
      "c:\\Users\\User\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\AppData\\Local\\llama_index\\models--microsoft--Phi-3.5-mini-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [03:01<00:00, 90.98s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "c:\\Users\\User\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--microsoft--Phi-3.5-mini-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [03:02<00:00, 91.38s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex , SimpleDirectoryReader, Settings, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Use a more advanced embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "# Create the tokenizer for Phi-3.5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    padding_side=\"right\",\n",
    "    pad_token=\"<|endoftext|>\"\n",
    ")\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "\n",
    "# Set up the Phi-3.5 model\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    tokenizer_name=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    context_window=128000,  # Phi-3.5 supports 128K context length\n",
    "    max_new_tokens=1024,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True, \"pad_token_id\": tokenizer.pad_token_id},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"../../data\").load_data()\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"Please write me essay about advantages and disadvantages of GPT? \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a huge variety of retrievers that you can learn about in our module guide on retrievers: https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
